{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fd37f6-fd2d-4c41-8f96-b0c7140d02db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Multiple Interfaces\n",
    "You can itneract with structured data (tables) multiple ways using Spark.\n",
    "\n",
    "We can interact with tables to transform data multiple ways:\n",
    "1. Executing SQL queries\n",
    "2. Working with the data using PySpark (python)\n",
    "3. Use SQL + PySpark (python) to create a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b0a630-0574-44b9-9f2c-d6374d11e89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Method 1: Executing SQL queries**\n",
    "\n",
    "This is a basic SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa22c17-f464-470a-89e3-8a845ba183a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- use spark SQL to get results from the orders table\n",
    "SELECT * FROM samples.tpch.orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d4af134-aba0-486a-8966-d8305e5f08e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Method 2: Working with the data using PySpark (python)**\n",
    "\n",
    "We can express queries using the Spark DataFrame API, which provides a Python-friendly, programmatic way to manipulate data.\n",
    "\n",
    "The following cell returns a DataFrame containing the same results as those retrieved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ed6d84-369b-468e-b7bc-854b46fcbbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the 'orders' table into a DataFrame\n",
    "df = spark.table('samples.tpch.orders')\n",
    "\n",
    "# Display the contents of the DataFrame called df\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "563ff754-6f28-4936-b6e6-20e939f3a7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  **Method 3: Use SQL + PySpark (python) to create a DataFrame**\n",
    "\n",
    "PySpark also supports running SQL queries directly. This is helpful for users coming from a SQL background or when you want to express complex logic in standard SQL syntax.\n",
    "\n",
    "You can run SQL queries with spark.sql(), and the result will be returned as a DataFrame that can be further transformed using PySpark (python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592050a0-c4fa-4ce0-976a-99b9b9279b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run a SQL query to retrieve data from the'orders' table\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      o_orderstatus,\n",
    "      year(o_orderdate) AS YEAR,\n",
    "      COUNT(*) AS order_count\n",
    "    FROM samples.tpch.orders\n",
    "    GROUP BY ALL\n",
    "\"\"\")\n",
    "\n",
    "# Display the results of the query\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c35cf3-0874-4168-b42e-9822860bcf7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a SQL Temporary View from a DataFrame**\n",
    "\n",
    "Once we have a DataFrame, we can register it as a temporary view using .`createOrReplaceTempView()`.\n",
    "This enables us to query the data using **SQL**, just like a table.\n",
    "\n",
    "This is especially powerful because it allows you to **seamlessly switch between Python and SQL within your notebook** — using PySpark (python) when you want programmatic control, and SQL when you want to write familiar declarative queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0013b2e-20c2-4e42-99e8-7c8dfa071b5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view from the orders DataFrame\n",
    "df.createOrReplaceTempView('order_status_by_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1ce2056-f1b7-400b-b4b8-77f8c3185816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Now you can query this dataframe, or temporary view, using SQL!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45cae51d-ffee-485c-a0d1-61f8d95232cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- use spark SQL to get results from the orders table\n",
    "SELECT * FROM order_status_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebce378e-9b8f-4550-bfbe-e4f55a439e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark DataFrames Are Not Modified In-Place\n",
    "In PySpark, DataFrames are not modified in-place. This means that when you apply a transformation — like filtering rows or adding a new column — the original DataFrame stays the same, and a new DataFrame is returned with the changes applied unless you redefine it!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8656ac5e-8609-4570-8e85-2df938462f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the orders table to a dataframe (df)\n",
    "df = spark.table(\"samples.tpch.orders\")\n",
    "\n",
    "# apply a transformation\n",
    "df.filter(df.o_orderstatus == 'F')\n",
    "\n",
    "# show that the original dataframe still contains all statuses\n",
    "display(df)\n",
    "\n",
    "# therefore you will often see syntax like this to redefine the dataframe using the query\n",
    "df = df.filter(df.o_orderstatus == 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aac02ef-2261-41ae-b25e-a053ff9b84d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Manipulation Using PySpark\n",
    "\n",
    "Now that we’ve explored how to load and query data using both **SQL and PySPark (python)**, the next step is to perform some light data manipulation using PySpark.\n",
    "\n",
    "PySpark provides a rich set of functions for transforming, cleaning, and enriching your data — all accessible through the `pyspark.sql.functions` module.\n",
    "\n",
    "We import this module with the alias `F` (i.e., `from pyspark.sql import functions as F`) for two main reasons:\n",
    "\n",
    "  ✅ It keeps our code clean and concise, especially when chaining multiple transformations.\n",
    "\n",
    "  ✅ It allows us to clearly distinguish between DataFrame column references (F.col(...)) and literal values (F.lit(...)), as well as access useful helpers like F.concat_ws, F.year, F.round, and more.\n",
    "\n",
    "In the following examples, we’ll apply a few common transformations to the `orders` table using this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f66cf4-909b-430f-97a5-ad3f275bcff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the orders table\n",
    "df = spark.table(\"samples.tpch.orders\")\n",
    "\n",
    "# Step 1: Filter for orders with status 'F' (fulfilled)\n",
    "df = df.filter(df.o_orderstatus == 'F')\n",
    "\n",
    "# Step 2: Add a new column that calculates estimated shipping delay (pretend logic)\n",
    "df = df.withColumn(\"estimated_delay_days\", F.lit(7))\n",
    "\n",
    "# Step 3: Concatenate order priority and status into a single label\n",
    "df = df.withColumn(\"priority_status\", F.concat_ws(\" - \", F.col(\"o_orderpriority\"), F.col(\"o_orderstatus\")))\n",
    "\n",
    "# Step 4: Rename o_orderdate to order_date for readability\n",
    "df = df.withColumnRenamed(\"o_orderdate\", \"order_date\")\n",
    "\n",
    "# Step 5: Add rounded_price column\n",
    "df = df.withColumn(\"rounded_price\", F.round(\"o_totalprice\", 0))\n",
    "\n",
    "# Step 6: Add a flag for high-value orders\n",
    "df = df.withColumn(\"is_high_value\", F.col(\"o_totalprice\") > 100000)\n",
    "\n",
    "# Step 7: Add year and month\n",
    "df = df.withColumn(\"order_year\", F.year(\"order_date\"))\n",
    "df = df.withColumn(\"order_month\", F.month(\"order_date\"))\n",
    "\n",
    "# Step 8: Add a ship deadline of 5 days\n",
    "df = df.withColumn(\"ship_deadline\", F.date_add(\"order_date\", 5))\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8f1e81e-1916-4884-8a08-25c53d6ca1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can also apply all of these transformations using a chained syntax, which keeps your code clean, readable, and efficient (and easier to read!).\n",
    "\n",
    "**The operations include:**\n",
    "\n",
    "🔍 Filtering for only fulfilled orders (o_orderstatus = 'F')\n",
    "\n",
    "🧮 Adding new columns such as a static shipping delay, a high-value flag, and a rounded price\n",
    "\n",
    "🧾 Concatenating columns to create a readable status label\n",
    "\n",
    "🗓️ Extracting date components like year and month from the order date\n",
    "\n",
    "📦 Renaming columns for clarity\n",
    "\n",
    "📅 Calculating a future shipping deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e39015a1-8ce2-441c-9c8d-3d18bed219ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform all transformations in a single chain\n",
    "df = (\n",
    "    spark.table(\"samples.tpch.orders\")\n",
    "    .filter(F.col(\"o_orderstatus\") == \"F\")  # Step 1: Filter for fulfilled orders\n",
    "    .withColumn(\"estimated_delay_days\", F.lit(7))  # Step 2: Add static column\n",
    "    .withColumn(\"priority_status\", F.concat_ws(\" - \", F.col(\"o_orderpriority\"), F.col(\"o_orderstatus\")))  # Step 3\n",
    "    .withColumnRenamed(\"o_orderdate\", \"order_date\")  # Step 4\n",
    "    .withColumn(\"rounded_price\", F.round(\"o_totalprice\", 0))  # Step 5\n",
    "    .withColumn(\"is_high_value\", F.col(\"o_totalprice\") > 100000)  # Step 6\n",
    "    .withColumn(\"order_year\", F.year(\"order_date\"))  # Step 7a\n",
    "    .withColumn(\"order_month\", F.month(\"order_date\"))  # Step 7b\n",
    "    .withColumn(\"ship_deadline\", F.date_add(\"order_date\", 5))  # Step 8\n",
    ")\n",
    "\n",
    "# Display the final result\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13061f85-4e5a-4392-827a-481f71f33876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Joining and Aggregating with SQL**\n",
    "\n",
    "Now that we’ve enriched our orders DataFrame with new fields, we’ll use Spark SQL to join it with the customer table. This allows us to analyze customer behavior across different market segments and order years.\n",
    "\n",
    "Specifically, we will:\n",
    "  1. Join orders with customer on the customer key\n",
    "  2. Group the results by mktsegment and order_year\n",
    "  3. Calculate:\n",
    "      - The **number of high-value orders**\n",
    "      - The **total order price** for each group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f422b3-47b3-4a65-af06-31b0f3def810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the transformed orders DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"transformed_orders\")\n",
    "\n",
    "# Use SQL to join with customer and perform aggregations\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.c_mktsegment AS mktsegment,\n",
    "        o.order_year,\n",
    "        COUNT(CASE WHEN o.is_high_value THEN 1 END) AS high_value_order_count,\n",
    "        SUM(o.o_totalprice) AS total_order_price\n",
    "    FROM transformed_orders o\n",
    "    JOIN samples.tpch.customer c\n",
    "        ON o.o_custkey = c.c_custkey\n",
    "    GROUP BY c.c_mktsegment, o.order_year\n",
    "    ORDER BY c.c_mktsegment, o.order_year\n",
    "\"\"\")\n",
    "\n",
    "# Display the results\n",
    "result_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_503a3ea9-ccd7-418f-a170-b8d4945d993e",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3698509841260356,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "pyspark for transforming data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
