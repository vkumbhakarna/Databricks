{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fd37f6-fd2d-4c41-8f96-b0c7140d02db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Multiple Interfaces\n",
    "You can itneract with structured data (tables) multiple ways using Spark.\n",
    "\n",
    "We can interact with tables to transform data multiple ways:\n",
    "1. Executing SQL queries\n",
    "2. Working with the data using PySpark (python)\n",
    "3. Use SQL + PySpark (python) to create a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b0a630-0574-44b9-9f2c-d6374d11e89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Method 1: Executing SQL queries**\n",
    "\n",
    "This is a basic SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa22c17-f464-470a-89e3-8a845ba183a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- use spark SQL to get results from the orders table\n",
    "SELECT * FROM samples.tpch.orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d4af134-aba0-486a-8966-d8305e5f08e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Method 2: Working with the data using PySpark (python)**\n",
    "\n",
    "We can express queries using the Spark DataFrame API, which provides a Python-friendly, programmatic way to manipulate data.\n",
    "\n",
    "The following cell returns a DataFrame containing the same results as those retrieved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ed6d84-369b-468e-b7bc-854b46fcbbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the 'orders' table into a DataFrame\n",
    "df = spark.table('samples.tpch.orders')\n",
    "\n",
    "# Display the contents of the DataFrame called df\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "563ff754-6f28-4936-b6e6-20e939f3a7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  **Method 3: Use SQL + PySpark (python) to create a DataFrame**\n",
    "\n",
    "PySpark also supports running SQL queries directly. This is helpful for users coming from a SQL background or when you want to express complex logic in standard SQL syntax.\n",
    "\n",
    "You can run SQL queries with spark.sql(), and the result will be returned as a DataFrame that can be further transformed using PySpark (python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592050a0-c4fa-4ce0-976a-99b9b9279b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run a SQL query to retrieve data from the'orders' table\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      o_orderstatus,\n",
    "      year(o_orderdate) AS YEAR,\n",
    "      COUNT(*) AS order_count\n",
    "    FROM samples.tpch.orders\n",
    "    GROUP BY ALL\n",
    "\"\"\")\n",
    "\n",
    "# Display the results of the query\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c35cf3-0874-4168-b42e-9822860bcf7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a SQL Temporary View from a DataFrame**\n",
    "\n",
    "Once we have a DataFrame, we can register it as a temporary view using .`createOrReplaceTempView()`.\n",
    "This enables us to query the data using **SQL**, just like a table.\n",
    "\n",
    "This is especially powerful because it allows you to **seamlessly switch between Python and SQL within your notebook** â€” using PySpark (python) when you want programmatic control, and SQL when you want to write familiar declarative queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0013b2e-20c2-4e42-99e8-7c8dfa071b5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view from the orders DataFrame\n",
    "df.createOrReplaceTempView('order_status_by_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1ce2056-f1b7-400b-b4b8-77f8c3185816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Now you can query this dataframe, or temporary view, using SQL!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45cae51d-ffee-485c-a0d1-61f8d95232cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- use spark SQL to get results from the orders table\n",
    "SELECT * FROM order_status_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebce378e-9b8f-4550-bfbe-e4f55a439e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark DataFrames Are Not Modified In-Place\n",
    "In PySpark, DataFrames are not modified in-place. This means that when you apply a transformation â€” like filtering rows or adding a new column â€” the original DataFrame stays the same, and a new DataFrame is returned with the changes applied unless you redefine it!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8656ac5e-8609-4570-8e85-2df938462f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the orders table to a dataframe (df)\n",
    "df = spark.table(\"samples.tpch.orders\")\n",
    "\n",
    "# apply a transformation\n",
    "df.filter(df.o_orderstatus == 'F')\n",
    "\n",
    "# show that the original dataframe still contains all statuses\n",
    "display(df)\n",
    "\n",
    "# therefore you will often see syntax like this to redefine the dataframe using the query\n",
    "df = df.filter(df.o_orderstatus == 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aac02ef-2261-41ae-b25e-a053ff9b84d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Manipulation Using PySpark\n",
    "\n",
    "Now that weâ€™ve explored how to load and query data using both **SQL and PySPark (python)**, the next step is to perform some light data manipulation using PySpark.\n",
    "\n",
    "PySpark provides a rich set of functions for transforming, cleaning, and enriching your data â€” all accessible through the `pyspark.sql.functions` module.\n",
    "\n",
    "We import this module with the alias `F` (i.e., `from pyspark.sql import functions as F`) for two main reasons:\n",
    "\n",
    "  âœ… It keeps our code clean and concise, especially when chaining multiple transformations.\n",
    "\n",
    "  âœ… It allows us to clearly distinguish between DataFrame column references (F.col(...)) and literal values (F.lit(...)), as well as access useful helpers like F.concat_ws, F.year, F.round, and more.\n",
    "\n",
    "In the following examples, weâ€™ll apply a few common transformations to the `orders` table using this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f66cf4-909b-430f-97a5-ad3f275bcff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the orders table\n",
    "df = spark.table(\"samples.tpch.orders\")\n",
    "\n",
    "# Step 1: Filter for orders with status 'F' (fulfilled)\n",
    "df = df.filter(df.o_orderstatus == 'F')\n",
    "\n",
    "# Step 2: Add a new column that calculates estimated shipping delay (pretend logic)\n",
    "df = df.withColumn(\"estimated_delay_days\", F.lit(7))\n",
    "\n",
    "# Step 3: Concatenate order priority and status into a single label\n",
    "df = df.withColumn(\"priority_status\", F.concat_ws(\" - \", F.col(\"o_orderpriority\"), F.col(\"o_orderstatus\")))\n",
    "\n",
    "# Step 4: Rename o_orderdate to order_date for readability\n",
    "df = df.withColumnRenamed(\"o_orderdate\", \"order_date\")\n",
    "\n",
    "# Step 5: Add rounded_price column\n",
    "df = df.withColumn(\"rounded_price\", F.round(\"o_totalprice\", 0))\n",
    "\n",
    "# Step 6: Add a flag for high-value orders\n",
    "df = df.withColumn(\"is_high_value\", F.col(\"o_totalprice\") > 100000)\n",
    "\n",
    "# Step 7: Add year and month\n",
    "df = df.withColumn(\"order_year\", F.year(\"order_date\"))\n",
    "df = df.withColumn(\"order_month\", F.month(\"order_date\"))\n",
    "\n",
    "# Step 8: Add a ship deadline of 5 days\n",
    "df = df.withColumn(\"ship_deadline\", F.date_add(\"order_date\", 5))\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8f1e81e-1916-4884-8a08-25c53d6ca1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can also apply all of these transformations using a chained syntax, which keeps your code clean, readable, and efficient (and easier to read!).\n",
    "\n",
    "**The operations include:**\n",
    "\n",
    "ðŸ” Filtering for only fulfilled orders (o_orderstatus = 'F')\n",
    "\n",
    "ðŸ§® Adding new columns such as a static shipping delay, a high-value flag, and a rounded price\n",
    "\n",
    "ðŸ§¾ Concatenating columns to create a readable status label\n",
    "\n",
    "ðŸ—“ï¸ Extracting date components like year and month from the order date\n",
    "\n",
    "ðŸ“¦ Renaming columns for clarity\n",
    "\n",
    "ðŸ“… Calculating a future shipping deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e39015a1-8ce2-441c-9c8d-3d18bed219ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform all transformations in a single chain\n",
    "df = (\n",
    "    spark.table(\"samples.tpch.orders\")\n",
    "    .filter(F.col(\"o_orderstatus\") == \"F\")  # Step 1: Filter for fulfilled orders\n",
    "    .withColumn(\"estimated_delay_days\", F.lit(7))  # Step 2: Add static column\n",
    "    .withColumn(\"priority_status\", F.concat_ws(\" - \", F.col(\"o_orderpriority\"), F.col(\"o_orderstatus\")))  # Step 3\n",
    "    .withColumnRenamed(\"o_orderdate\", \"order_date\")  # Step 4\n",
    "    .withColumn(\"rounded_price\", F.round(\"o_totalprice\", 0))  # Step 5\n",
    "    .withColumn(\"is_high_value\", F.col(\"o_totalprice\") > 100000)  # Step 6\n",
    "    .withColumn(\"order_year\", F.year(\"order_date\"))  # Step 7a\n",
    "    .withColumn(\"order_month\", F.month(\"order_date\"))  # Step 7b\n",
    "    .withColumn(\"ship_deadline\", F.date_add(\"order_date\", 5))  # Step 8\n",
    ")\n",
    "\n",
    "# Display the final result\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13061f85-4e5a-4392-827a-481f71f33876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Joining and Aggregating with SQL**\n",
    "\n",
    "Now that weâ€™ve enriched our orders DataFrame with new fields, weâ€™ll use Spark SQL to join it with the customer table. This allows us to analyze customer behavior across different market segments and order years.\n",
    "\n",
    "Specifically, we will:\n",
    "  1. Join orders with customer on the customer key\n",
    "  2. Group the results by mktsegment and order_year\n",
    "  3. Calculate:\n",
    "      - The **number of high-value orders**\n",
    "      - The **total order price** for each group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f422b3-47b3-4a65-af06-31b0f3def810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the transformed orders DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"transformed_orders\")\n",
    "\n",
    "# Use SQL to join with customer and perform aggregations\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.c_mktsegment AS mktsegment,\n",
    "        o.order_year,\n",
    "        COUNT(CASE WHEN o.is_high_value THEN 1 END) AS high_value_order_count,\n",
    "        SUM(o.o_totalprice) AS total_order_price\n",
    "    FROM transformed_orders o\n",
    "    JOIN samples.tpch.customer c\n",
    "        ON o.o_custkey = c.c_custkey\n",
    "    GROUP BY c.c_mktsegment, o.order_year\n",
    "    ORDER BY c.c_mktsegment, o.order_year\n",
    "\"\"\")\n",
    "\n",
    "# Display the results\n",
    "result_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_503a3ea9-ccd7-418f-a170-b8d4945d993e",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3698509841260356,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "pyspark for transforming data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
